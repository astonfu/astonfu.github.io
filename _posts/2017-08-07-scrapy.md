---
layout: post
title: Scrapy 爬虫框架
permalink: scrapy
---

# 简介

Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。

其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试


Scrapy 使用了 Twisted异步网络库来处理网络通讯。

框架图如下：

![框架图](/img/ScrapyArchitecture.png)

可见 Spider 与 Scheduler 及 Downloader 之间形成里很好的循环。从第一页面，Spider 从 Downloader 得到 response 解析出 下一个 url，交给 Scheduler 处理形成新当 request。而且三者之间的交互都有中间件，可以插入 hook 代码，对过程进行控制和处理。而 Item 与 Itempipeline 合作，对数据进行持久化等处理。

Scrapy主要包括了以下组件：

- 引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)。
- 调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址。
- 下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)。
- 爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面。
- 项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
-下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
-爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
-调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。


# 一个入门例子

### 初始化一个项目

```bash
scrapy startproject tutorial
```

在 spiders 目录里写一个爬虫：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

说明：

- name: 爬虫的名字。
- start_requests(): 返回一个可以被遍历的 Request 实例。
- parse(): 当获得 request 时调用解析 response 当内容。一般它会返回字典格式当爬取当内容，同时找到下一个要爬取当地址并创建 Request。

### 执行

```bash
scrapy crawl quotes
```

就会按照 parse 里的过程保存两个 html 文件了。


### 提取

用 shell 来交互提取 data，方便找到想要的内容当提取表达式：

```bash
scrapy shell 'http://quotes.toscrape.com/page/1/'

>>> response.css('title')
[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]

>>> response.css('title::text').extract()
['Quotes to Scrape']
>>> response.css('title::text').extract_first()
'Quotes to Scrape'
>>> response.css('title::text')[0].extract()
'Quotes to Scrape'

>>> response.css('title::text').re(r'Quotes.*')
['Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
['Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
['Quotes', 'Scrape']
```


一个爬虫一般都会输出从页面里摘取包含数据当字典，这里我们用 Python 当 yield.

上一改例子：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.par
            # 用 follow 的写法，就不用拼出全部 url 了
            # yield response.follow(next_page, callback=self.parse)
```


### 存储

```bash
# JSON 格式
scrapy crawl quotes -o quotes.json
# JSON Line 格式
scrapy crawl quotes -o quotes.jl
```

# 参考
- https://docs.scrapy.org/en/latest/intro/tutorial.html
- http://www.jianshu.com/p/078ad2067419
